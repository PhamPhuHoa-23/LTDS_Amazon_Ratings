{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing - Amazon Beauty Products Recommendation System\n",
    "\n",
    "**CSC17104 - Programming for Data Science**  \n",
    "**Student:** Angela - MSSV: 23122030  \n",
    "**Notebook:** 02_preprocessing.ipynb\n",
    "\n",
    "---\n",
    "\n",
    "## Mục tiêu\n",
    "\n",
    "Notebook này thực hiện preprocessing dữ liệu Amazon Beauty ratings với các bước:\n",
    "\n",
    "1. **Xử lý Missing Values** - Phát hiện và xử lý dữ liệu thiếu\n",
    "2. **Outlier Detection** - Loại bỏ dữ liệu bất thường\n",
    "3. **Feature Engineering** - Tạo features mới từ dữ liệu hiện có\n",
    "4. **Normalization/Standardization** - Chuẩn hóa dữ liệu\n",
    "5. **Data Filtering** - Lọc users/products có ít ratings\n",
    "6. **Save Processed Data** - Lưu dữ liệu đã xử lý\n",
    "\n",
    "**Tối ưu hóa:** Tất cả operations đều vectorized (không dùng loops) để chạy nhanh.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries và Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"AMAZON BEAUTY PRODUCTS - DATA PREPROCESSING\")\n",
    "print(\"Vectorized Implementation for Performance\")\n",
    "print(\"=\"*80)\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Raw Data (Vectorized)\n",
    "\n",
    "**Kỹ thuật:** Dùng `np.genfromtxt()` để load toàn bộ file một lần (vectorized I/O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[BƯỚC 1] Đọc dữ liệu thô...\")\n",
    "\n",
    "data_path = '../data/raw/ratings_Beauty.csv'\n",
    "\n",
    "# Vectorized loading - đọc toàn bộ file một lần\n",
    "data = np.genfromtxt(\n",
    "    data_path,\n",
    "    delimiter=',',\n",
    "    skip_header=1,\n",
    "    dtype=None,\n",
    "    encoding='utf-8',\n",
    "    names=['UserId', 'ProductId', 'Rating', 'Timestamp']\n",
    ")\n",
    "\n",
    "# Extract arrays (vectorized operation - không cần loop)\n",
    "user_ids = data['UserId']\n",
    "product_ids = data['ProductId']\n",
    "ratings = data['Rating'].astype(np.float64)\n",
    "timestamps = data['Timestamp'].astype(np.int64)\n",
    "\n",
    "n_records = len(data)\n",
    "\n",
    "print(f\"✓ Đã load {n_records:,} bản ghi\")\n",
    "print(f\"  Memory usage: {data.nbytes / 1024**2:.2f} MB\")\n",
    "print(f\"\\nData shape: {data.shape}\")\n",
    "print(f\"Columns: UserId, ProductId, Rating, Timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Missing Value Analysis (Vectorized)\n",
    "\n",
    "**Kỹ thuật:** Dùng boolean indexing và `np.sum()` thay vì loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[BƯỚC 2] Phân tích và xử lý missing values...\")\n",
    "\n",
    "# Vectorized NaN detection - tính toán song song trên toàn bộ array\n",
    "nan_ratings = np.isnan(ratings)\n",
    "nan_timestamps = timestamps == 0  # Timestamp = 0 cũng là invalid\n",
    "\n",
    "n_nan_ratings = np.sum(nan_ratings)  # Vectorized count\n",
    "n_nan_timestamps = np.sum(nan_timestamps)\n",
    "\n",
    "print(f\"Missing values:\")\n",
    "print(f\"  - Rating: {n_nan_ratings:,} ({n_nan_ratings/n_records*100:.2f}%)\")\n",
    "print(f\"  - Timestamp: {n_nan_timestamps:,} ({n_nan_timestamps/n_records*100:.2f}%)\")\n",
    "\n",
    "# Strategy: Vectorized filtering - loại bỏ hàng có missing critical values\n",
    "# Boolean indexing (vectorized) thay vì loop qua từng hàng\n",
    "valid_mask = ~nan_ratings & ~nan_timestamps\n",
    "\n",
    "if np.sum(~valid_mask) > 0:\n",
    "    # Apply mask vectorized - tất cả arrays cùng lúc\n",
    "    user_ids = user_ids[valid_mask]\n",
    "    product_ids = product_ids[valid_mask]\n",
    "    ratings = ratings[valid_mask]\n",
    "    timestamps = timestamps[valid_mask]\n",
    "    \n",
    "    removed = np.sum(~valid_mask)\n",
    "    print(f\"\\n✓ Đã loại bỏ {removed:,} hàng có missing values\")\n",
    "    print(f\"  Còn lại: {len(ratings):,} bản ghi ({len(ratings)/n_records*100:.2f}%)\")\n",
    "else:\n",
    "    print(\"\\n✓ Không có missing values\")\n",
    "\n",
    "# Update record count\n",
    "n_records = len(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Outlier Detection (Vectorized)\n",
    "\n",
    "**Phương pháp:** \n",
    "- Kiểm tra rating range (1.0 - 5.0)\n",
    "- Kiểm tra timestamp hợp lệ\n",
    "- Dùng boolean masks cho tất cả operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[BƯỚC 3] Phát hiện và xử lý outliers...\")\n",
    "\n",
    "# Rating validation (vectorized comparison)\n",
    "valid_rating_mask = (ratings >= 1.0) & (ratings <= 5.0)\n",
    "invalid_ratings = np.sum(~valid_rating_mask)\n",
    "\n",
    "print(f\"\\nRating validation:\")\n",
    "print(f\"  Range: [{np.min(ratings):.1f}, {np.max(ratings):.1f}]\")\n",
    "print(f\"  Invalid ratings (ngoài 1-5): {invalid_ratings:,}\")\n",
    "\n",
    "if invalid_ratings > 0:\n",
    "    # Vectorized filtering\n",
    "    user_ids = user_ids[valid_rating_mask]\n",
    "    product_ids = product_ids[valid_rating_mask]\n",
    "    ratings = ratings[valid_rating_mask]\n",
    "    timestamps = timestamps[valid_rating_mask]\n",
    "    print(f\"  ✓ Đã loại bỏ {invalid_ratings:,} ratings không hợp lệ\")\n",
    "\n",
    "# Timestamp validation (vectorized)\n",
    "min_timestamp = 946684800   # 2000-01-01 00:00:00\n",
    "max_timestamp = 1704067200  # 2024-01-01 00:00:00\n",
    "valid_ts_mask = (timestamps >= min_timestamp) & (timestamps <= max_timestamp)\n",
    "invalid_timestamps = np.sum(~valid_ts_mask)\n",
    "\n",
    "print(f\"\\nTimestamp validation:\")\n",
    "print(f\"  Valid range: 2000-01-01 to 2024-01-01\")\n",
    "print(f\"  Invalid timestamps: {invalid_timestamps:,}\")\n",
    "\n",
    "if invalid_timestamps > 0:\n",
    "    user_ids = user_ids[valid_ts_mask]\n",
    "    product_ids = product_ids[valid_ts_mask]\n",
    "    ratings = ratings[valid_ts_mask]\n",
    "    timestamps = timestamps[valid_ts_mask]\n",
    "    print(f\"  ✓ Đã loại bỏ {invalid_timestamps:,} timestamps không hợp lệ\")\n",
    "\n",
    "print(f\"\\n✓ Sau khi xử lý outliers: {len(ratings):,} bản ghi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering (Vectorized)\n",
    "\n",
    "**Features mới:**\n",
    "1. **User features** - Tính toán cho mỗi user (vectorized groupby)\n",
    "2. **Product features** - Tính toán cho mỗi product\n",
    "3. **Temporal features** - Extract từ timestamp\n",
    "4. **Interaction features** - User-product interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[BƯỚC 4] Feature Engineering (Vectorized)...\")\n",
    "\n",
    "# ===== 5.1: User Features (Vectorized) =====\n",
    "print(\"\\n5.1 User Features:\")\n",
    "\n",
    "# Get unique users and create mapping\n",
    "unique_users, user_inverse = np.unique(user_ids, return_inverse=True)\n",
    "n_users = len(unique_users)\n",
    "\n",
    "# Vectorized computation: ratings per user\n",
    "# Dùng np.bincount thay vì loop - cực kỳ nhanh\n",
    "user_rating_counts = np.bincount(user_inverse)\n",
    "\n",
    "# Vectorized computation: average rating per user\n",
    "# Dùng np.bincount với weights\n",
    "user_rating_sums = np.bincount(user_inverse, weights=ratings)\n",
    "user_avg_ratings = user_rating_sums / np.maximum(user_rating_counts, 1)  # Tránh chia 0\n",
    "\n",
    "# Vectorized computation: rating variance per user\n",
    "# Tính variance bằng E[X^2] - (E[X])^2\n",
    "user_rating_sq_sums = np.bincount(user_inverse, weights=ratings**2)\n",
    "user_rating_vars = (user_rating_sq_sums / np.maximum(user_rating_counts, 1)) - user_avg_ratings**2\n",
    "user_rating_stds = np.sqrt(np.maximum(user_rating_vars, 0))  # Tránh sqrt của số âm (numerical error)\n",
    "\n",
    "# Map features back to original data (vectorized indexing)\n",
    "user_n_ratings = user_rating_counts[user_inverse]\n",
    "user_mean_rating = user_avg_ratings[user_inverse]\n",
    "user_std_rating = user_rating_stds[user_inverse]\n",
    "\n",
    "print(f\"  Unique users: {n_users:,}\")\n",
    "print(f\"  Features created:\")\n",
    "print(f\"    - user_n_ratings (số lượng ratings của user)\")\n",
    "print(f\"    - user_mean_rating (trung bình rating của user)\")\n",
    "print(f\"    - user_std_rating (độ lệch chuẩn rating của user)\")\n",
    "print(f\"  ✓ Completed (vectorized operations)\")\n",
    "\n",
    "# ===== 5.2: Product Features (Vectorized) =====\n",
    "print(\"\\n5.2 Product Features:\")\n",
    "\n",
    "# Get unique products and create mapping\n",
    "unique_products, product_inverse = np.unique(product_ids, return_inverse=True)\n",
    "n_products = len(unique_products)\n",
    "\n",
    "# Vectorized computation: ratings per product\n",
    "product_rating_counts = np.bincount(product_inverse)\n",
    "\n",
    "# Vectorized computation: average rating per product\n",
    "product_rating_sums = np.bincount(product_inverse, weights=ratings)\n",
    "product_avg_ratings = product_rating_sums / np.maximum(product_rating_counts, 1)\n",
    "\n",
    "# Vectorized computation: rating variance per product\n",
    "product_rating_sq_sums = np.bincount(product_inverse, weights=ratings**2)\n",
    "product_rating_vars = (product_rating_sq_sums / np.maximum(product_rating_counts, 1)) - product_avg_ratings**2\n",
    "product_rating_stds = np.sqrt(np.maximum(product_rating_vars, 0))\n",
    "\n",
    "# Map features back to original data\n",
    "product_n_ratings = product_rating_counts[product_inverse]\n",
    "product_mean_rating = product_avg_ratings[product_inverse]\n",
    "product_std_rating = product_rating_stds[product_inverse]\n",
    "\n",
    "print(f\"  Unique products: {n_products:,}\")\n",
    "print(f\"  Features created:\")\n",
    "print(f\"    - product_n_ratings (số lượng ratings của product)\")\n",
    "print(f\"    - product_mean_rating (trung bình rating của product)\")\n",
    "print(f\"    - product_std_rating (độ lệch chuẩn rating của product)\")\n",
    "print(f\"  ✓ Completed (vectorized operations)\")\n",
    "\n",
    "# ===== 5.3: Temporal Features (Vectorized) =====\n",
    "print(\"\\n5.3 Temporal Features:\")\n",
    "\n",
    "# Convert timestamps to datetime components (vectorized)\n",
    "# Sử dụng numpy datetime64 cho vectorization\n",
    "datetime_array = timestamps.astype('datetime64[s]')\n",
    "\n",
    "# Extract year, month, day (vectorized)\n",
    "years = datetime_array.astype('datetime64[Y]').astype(int) + 1970\n",
    "months = datetime_array.astype('datetime64[M]').astype(int) % 12 + 1\n",
    "\n",
    "# Day of week (0=Monday, 6=Sunday) - vectorized\n",
    "# Dùng công thức Zeller's congruence hoặc numpy datetime\n",
    "weekdays = (datetime_array.astype('datetime64[D]').view('int64') - 4) % 7\n",
    "\n",
    "# Recency: days since last timestamp (vectorized)\n",
    "max_timestamp = np.max(timestamps)\n",
    "days_since = (max_timestamp - timestamps) / (24 * 3600)  # Convert to days\n",
    "\n",
    "# Recency weight (exponential decay) - vectorized\n",
    "# Recent ratings có weight cao hơn\n",
    "recency_weight = np.exp(-days_since / 365.0)  # Decay với half-life 1 năm\n",
    "\n",
    "print(f\"  Features created:\")\n",
    "print(f\"    - year (năm rating)\")\n",
    "print(f\"    - month (tháng rating)\")\n",
    "print(f\"    - weekday (ngày trong tuần)\")\n",
    "print(f\"    - days_since (số ngày kể từ rating cuối)\")\n",
    "print(f\"    - recency_weight (trọng số theo thời gian)\")\n",
    "print(f\"  ✓ Completed (vectorized operations)\")\n",
    "\n",
    "# ===== 5.4: Interaction Features (Vectorized) =====\n",
    "print(\"\\n5.4 Interaction Features:\")\n",
    "\n",
    "# User deviation: user's rating - user's average (vectorized)\n",
    "user_rating_deviation = ratings - user_mean_rating\n",
    "\n",
    "# Product deviation: user's rating - product's average (vectorized)\n",
    "product_rating_deviation = ratings - product_mean_rating\n",
    "\n",
    "# Global deviation: user's rating - global average (vectorized)\n",
    "global_mean_rating = np.mean(ratings)\n",
    "global_rating_deviation = ratings - global_mean_rating\n",
    "\n",
    "# Z-score normalization (vectorized)\n",
    "# Chuẩn hóa rating theo user's distribution\n",
    "user_rating_zscore = user_rating_deviation / np.maximum(user_std_rating, 0.01)  # Tránh chia 0\n",
    "\n",
    "print(f\"  Features created:\")\n",
    "print(f\"    - user_rating_deviation (rating - user avg)\")\n",
    "print(f\"    - product_rating_deviation (rating - product avg)\")\n",
    "print(f\"    - global_rating_deviation (rating - global avg)\")\n",
    "print(f\"    - user_rating_zscore (z-score normalized rating)\")\n",
    "print(f\"  Global mean rating: {global_mean_rating:.4f}\")\n",
    "print(f\"  ✓ Completed (vectorized operations)\")\n",
    "\n",
    "print(f\"\\n✓ Feature Engineering hoàn tất!\")\n",
    "print(f\"  Tổng số features: 15 features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Statistics Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize engineered features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# User activity distribution\n",
    "axes[0, 0].hist(user_n_ratings, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Number of Ratings per User')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('User Activity Distribution')\n",
    "axes[0, 0].set_yscale('log')\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# User mean rating distribution\n",
    "axes[0, 1].hist(user_mean_rating, bins=50, color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('User Mean Rating')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('User Rating Behavior')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Product popularity distribution\n",
    "axes[0, 2].hist(product_n_ratings, bins=50, color='seagreen', edgecolor='black', alpha=0.7)\n",
    "axes[0, 2].set_xlabel('Number of Ratings per Product')\n",
    "axes[0, 2].set_ylabel('Frequency')\n",
    "axes[0, 2].set_title('Product Popularity Distribution')\n",
    "axes[0, 2].set_yscale('log')\n",
    "axes[0, 2].grid(alpha=0.3)\n",
    "\n",
    "# Temporal trend\n",
    "year_counts = np.bincount(years - np.min(years))\n",
    "year_labels = np.arange(np.min(years), np.max(years) + 1)\n",
    "axes[1, 0].plot(year_labels[:len(year_counts)], year_counts, marker='o', linewidth=2, color='darkviolet')\n",
    "axes[1, 0].set_xlabel('Year')\n",
    "axes[1, 0].set_ylabel('Number of Ratings')\n",
    "axes[1, 0].set_title('Rating Activity Over Time')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Rating deviation distribution\n",
    "axes[1, 1].hist(user_rating_deviation, bins=50, color='orange', edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('User Rating Deviation')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('User Rating Deviation from Mean')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "# Recency weight distribution\n",
    "axes[1, 2].hist(recency_weight, bins=50, color='teal', edgecolor='black', alpha=0.7)\n",
    "axes[1, 2].set_xlabel('Recency Weight')\n",
    "axes[1, 2].set_ylabel('Frequency')\n",
    "axes[1, 2].set_title('Recency Weight Distribution')\n",
    "axes[1, 2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFeature statistics summary:\")\n",
    "print(f\"  User ratings: mean={np.mean(user_n_ratings):.2f}, median={np.median(user_n_ratings):.0f}\")\n",
    "print(f\"  Product ratings: mean={np.mean(product_n_ratings):.2f}, median={np.median(product_n_ratings):.0f}\")\n",
    "print(f\"  User avg rating: mean={np.mean(user_mean_rating):.3f}, std={np.std(user_mean_rating):.3f}\")\n",
    "print(f\"  Product avg rating: mean={np.mean(product_mean_rating):.3f}, std={np.std(product_mean_rating):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Normalization (Vectorized)\n",
    "\n",
    "**Phương pháp:**\n",
    "1. **Min-Max Normalization** - Scale về [0, 1]\n",
    "2. **Z-score Standardization** - Mean=0, Std=1\n",
    "3. **Robust Scaling** - Dùng median và IQR (robust to outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[BƯỚC 5] Normalization và Standardization...\")\n",
    "\n",
    "# ===== 7.1: Min-Max Normalization (Vectorized) =====\n",
    "print(\"\\n7.1 Min-Max Normalization:\")\n",
    "\n",
    "def minmax_normalize(x):\n",
    "    \"\"\"Vectorized Min-Max normalization về [0, 1]\"\"\"\n",
    "    x_min = np.min(x)\n",
    "    x_max = np.max(x)\n",
    "    if x_max - x_min == 0:\n",
    "        return np.zeros_like(x)\n",
    "    return (x - x_min) / (x_max - x_min)\n",
    "\n",
    "# Normalize features (vectorized)\n",
    "user_n_ratings_norm = minmax_normalize(user_n_ratings.astype(np.float64))\n",
    "product_n_ratings_norm = minmax_normalize(product_n_ratings.astype(np.float64))\n",
    "days_since_norm = minmax_normalize(days_since)\n",
    "\n",
    "print(f\"  Normalized features:\")\n",
    "print(f\"    - user_n_ratings_norm: [{np.min(user_n_ratings_norm):.3f}, {np.max(user_n_ratings_norm):.3f}]\")\n",
    "print(f\"    - product_n_ratings_norm: [{np.min(product_n_ratings_norm):.3f}, {np.max(product_n_ratings_norm):.3f}]\")\n",
    "print(f\"    - days_since_norm: [{np.min(days_since_norm):.3f}, {np.max(days_since_norm):.3f}]\")\n",
    "\n",
    "# ===== 7.2: Z-score Standardization (Vectorized) =====\n",
    "print(\"\\n7.2 Z-score Standardization:\")\n",
    "\n",
    "def zscore_standardize(x):\n",
    "    \"\"\"Vectorized Z-score standardization\"\"\"\n",
    "    mean = np.mean(x)\n",
    "    std = np.std(x)\n",
    "    if std == 0:\n",
    "        return np.zeros_like(x)\n",
    "    return (x - mean) / std\n",
    "\n",
    "# Standardize features (vectorized)\n",
    "ratings_std = zscore_standardize(ratings)\n",
    "user_mean_rating_std = zscore_standardize(user_mean_rating)\n",
    "product_mean_rating_std = zscore_standardize(product_mean_rating)\n",
    "\n",
    "print(f\"  Standardized features (mean ≈ 0, std ≈ 1):\")\n",
    "print(f\"    - ratings_std: mean={np.mean(ratings_std):.6f}, std={np.std(ratings_std):.6f}\")\n",
    "print(f\"    - user_mean_rating_std: mean={np.mean(user_mean_rating_std):.6f}, std={np.std(user_mean_rating_std):.6f}\")\n",
    "print(f\"    - product_mean_rating_std: mean={np.mean(product_mean_rating_std):.6f}, std={np.std(product_mean_rating_std):.6f}\")\n",
    "\n",
    "# ===== 7.3: Robust Scaling (Vectorized) =====\n",
    "print(\"\\n7.3 Robust Scaling (using median and IQR):\")\n",
    "\n",
    "def robust_scale(x):\n",
    "    \"\"\"Vectorized Robust scaling using median and IQR\"\"\"\n",
    "    median = np.median(x)\n",
    "    q75 = np.percentile(x, 75)\n",
    "    q25 = np.percentile(x, 25)\n",
    "    iqr = q75 - q25\n",
    "    if iqr == 0:\n",
    "        return np.zeros_like(x)\n",
    "    return (x - median) / iqr\n",
    "\n",
    "# Robust scaling (vectorized)\n",
    "user_n_ratings_robust = robust_scale(user_n_ratings.astype(np.float64))\n",
    "product_n_ratings_robust = robust_scale(product_n_ratings.astype(np.float64))\n",
    "\n",
    "print(f\"  Robust scaled features:\")\n",
    "print(f\"    - user_n_ratings_robust: median={np.median(user_n_ratings_robust):.3f}\")\n",
    "print(f\"    - product_n_ratings_robust: median={np.median(product_n_ratings_robust):.3f}\")\n",
    "\n",
    "print(f\"\\n✓ Normalization/Standardization hoàn tất!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Filtering (Vectorized)\n",
    "\n",
    "**Mục đích:** Giảm sparsity bằng cách loại bỏ users/products có quá ít ratings\n",
    "\n",
    "**Strategy:**\n",
    "- Filter users với < min_user_ratings\n",
    "- Filter products với < min_product_ratings\n",
    "- Iterative filtering cho đến khi stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[BƯỚC 6] Data Filtering để giảm sparsity...\")\n",
    "\n",
    "# Thresholds\n",
    "MIN_USER_RATINGS = 5    # User phải có ít nhất 5 ratings\n",
    "MIN_PRODUCT_RATINGS = 5  # Product phải có ít nhất 5 ratings\n",
    "\n",
    "print(f\"\\nFiltering criteria:\")\n",
    "print(f\"  - Min ratings per user: {MIN_USER_RATINGS}\")\n",
    "print(f\"  - Min ratings per product: {MIN_PRODUCT_RATINGS}\")\n",
    "\n",
    "# Before filtering stats\n",
    "print(f\"\\nBefore filtering:\")\n",
    "print(f\"  - Records: {len(ratings):,}\")\n",
    "print(f\"  - Users: {len(np.unique(user_ids)):,}\")\n",
    "print(f\"  - Products: {len(np.unique(product_ids)):,}\")\n",
    "\n",
    "# Iterative filtering (vectorized)\n",
    "iteration = 0\n",
    "prev_n_records = len(ratings)\n",
    "\n",
    "while True:\n",
    "    iteration += 1\n",
    "    \n",
    "    # Count ratings per user (vectorized)\n",
    "    unique_users_temp, user_inverse_temp = np.unique(user_ids, return_inverse=True)\n",
    "    user_counts_temp = np.bincount(user_inverse_temp)\n",
    "    \n",
    "    # Count ratings per product (vectorized)\n",
    "    unique_products_temp, product_inverse_temp = np.unique(product_ids, return_inverse=True)\n",
    "    product_counts_temp = np.bincount(product_inverse_temp)\n",
    "    \n",
    "    # Create masks (vectorized)\n",
    "    user_valid = user_counts_temp[user_inverse_temp] >= MIN_USER_RATINGS\n",
    "    product_valid = product_counts_temp[product_inverse_temp] >= MIN_PRODUCT_RATINGS\n",
    "    \n",
    "    # Combined mask\n",
    "    valid_mask = user_valid & product_valid\n",
    "    \n",
    "    # Apply filter (vectorized)\n",
    "    user_ids = user_ids[valid_mask]\n",
    "    product_ids = product_ids[valid_mask]\n",
    "    ratings = ratings[valid_mask]\n",
    "    timestamps = timestamps[valid_mask]\n",
    "    \n",
    "    # Update all feature arrays\n",
    "    user_n_ratings = user_n_ratings[valid_mask]\n",
    "    user_mean_rating = user_mean_rating[valid_mask]\n",
    "    user_std_rating = user_std_rating[valid_mask]\n",
    "    product_n_ratings = product_n_ratings[valid_mask]\n",
    "    product_mean_rating = product_mean_rating[valid_mask]\n",
    "    product_std_rating = product_std_rating[valid_mask]\n",
    "    years = years[valid_mask]\n",
    "    months = months[valid_mask]\n",
    "    weekdays = weekdays[valid_mask]\n",
    "    days_since = days_since[valid_mask]\n",
    "    recency_weight = recency_weight[valid_mask]\n",
    "    user_rating_deviation = user_rating_deviation[valid_mask]\n",
    "    product_rating_deviation = product_rating_deviation[valid_mask]\n",
    "    global_rating_deviation = global_rating_deviation[valid_mask]\n",
    "    user_rating_zscore = user_rating_zscore[valid_mask]\n",
    "    \n",
    "    # Normalized features\n",
    "    user_n_ratings_norm = user_n_ratings_norm[valid_mask]\n",
    "    product_n_ratings_norm = product_n_ratings_norm[valid_mask]\n",
    "    days_since_norm = days_since_norm[valid_mask]\n",
    "    ratings_std = ratings_std[valid_mask]\n",
    "    user_mean_rating_std = user_mean_rating_std[valid_mask]\n",
    "    product_mean_rating_std = product_mean_rating_std[valid_mask]\n",
    "    user_n_ratings_robust = user_n_ratings_robust[valid_mask]\n",
    "    product_n_ratings_robust = product_n_ratings_robust[valid_mask]\n",
    "    \n",
    "    n_removed = prev_n_records - len(ratings)\n",
    "    print(f\"  Iteration {iteration}: Removed {n_removed:,} records\")\n",
    "    \n",
    "    # Check convergence\n",
    "    if n_removed == 0:\n",
    "        print(f\"  ✓ Converged after {iteration} iterations\")\n",
    "        break\n",
    "    \n",
    "    prev_n_records = len(ratings)\n",
    "    \n",
    "    if iteration > 10:\n",
    "        print(f\"  ⚠ Max iterations reached\")\n",
    "        break\n",
    "\n",
    "# After filtering stats\n",
    "final_users = np.unique(user_ids)\n",
    "final_products = np.unique(product_ids)\n",
    "\n",
    "print(f\"\\nAfter filtering:\")\n",
    "print(f\"  - Records: {len(ratings):,}\")\n",
    "print(f\"  - Users: {len(final_users):,}\")\n",
    "print(f\"  - Products: {len(final_products):,}\")\n",
    "\n",
    "# Sparsity calculation\n",
    "total_possible = len(final_users) * len(final_products)\n",
    "sparsity = 1 - (len(ratings) / total_possible)\n",
    "print(f\"\\nSparsity after filtering: {sparsity:.6f} ({sparsity*100:.4f}%)\")\n",
    "print(f\"Density: {(1-sparsity)*100:.6f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create ID Mappings (Vectorized)\n",
    "\n",
    "**Mục đích:** Map string IDs → integer indices để dùng trong matrix operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[BƯỚC 7] Tạo ID mappings...\")\n",
    "\n",
    "# Create user mapping (vectorized)\n",
    "unique_users_final, user_indices = np.unique(user_ids, return_inverse=True)\n",
    "n_users_final = len(unique_users_final)\n",
    "\n",
    "# Create product mapping (vectorized)\n",
    "unique_products_final, product_indices = np.unique(product_ids, return_inverse=True)\n",
    "n_products_final = len(unique_products_final)\n",
    "\n",
    "print(f\"User mapping:\")\n",
    "print(f\"  - Original IDs: strings (e.g., '{unique_users_final[0]}')\")\n",
    "print(f\"  - Mapped IDs: integers [0, {n_users_final-1}]\")\n",
    "print(f\"  - Total users: {n_users_final:,}\")\n",
    "\n",
    "print(f\"\\nProduct mapping:\")\n",
    "print(f\"  - Original IDs: strings (e.g., '{unique_products_final[0]}')\")\n",
    "print(f\"  - Mapped IDs: integers [0, {n_products_final-1}]\")\n",
    "print(f\"  - Total products: {n_products_final:,}\")\n",
    "\n",
    "print(f\"\\n✓ ID mappings created (vectorized)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPROCESSING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. DATA SIZE:\")\n",
    "print(f\"   - Final records: {len(ratings):,}\")\n",
    "print(f\"   - Final users: {n_users_final:,}\")\n",
    "print(f\"   - Final products: {n_products_final:,}\")\n",
    "print(f\"   - Sparsity: {sparsity*100:.4f}%\")\n",
    "\n",
    "print(f\"\\n2. RATING STATISTICS:\")\n",
    "print(f\"   - Mean: {np.mean(ratings):.4f}\")\n",
    "print(f\"   - Median: {np.median(ratings):.1f}\")\n",
    "print(f\"   - Std: {np.std(ratings):.4f}\")\n",
    "print(f\"   - Range: [{np.min(ratings):.1f}, {np.max(ratings):.1f}]\")\n",
    "\n",
    "print(f\"\\n3. USER STATISTICS:\")\n",
    "unique_u, u_counts = np.unique(user_indices, return_counts=True)\n",
    "print(f\"   - Avg ratings/user: {np.mean(u_counts):.2f}\")\n",
    "print(f\"   - Median ratings/user: {np.median(u_counts):.0f}\")\n",
    "print(f\"   - Min ratings/user: {np.min(u_counts)}\")\n",
    "print(f\"   - Max ratings/user: {np.max(u_counts)}\")\n",
    "\n",
    "print(f\"\\n4. PRODUCT STATISTICS:\")\n",
    "unique_p, p_counts = np.unique(product_indices, return_counts=True)\n",
    "print(f\"   - Avg ratings/product: {np.mean(p_counts):.2f}\")\n",
    "print(f\"   - Median ratings/product: {np.median(p_counts):.0f}\")\n",
    "print(f\"   - Min ratings/product: {np.min(p_counts)}\")\n",
    "print(f\"   - Max ratings/product: {np.max(p_counts)}\")\n",
    "\n",
    "print(f\"\\n5. FEATURES CREATED:\")\n",
    "print(f\"   - User features: 3 (n_ratings, mean_rating, std_rating)\")\n",
    "print(f\"   - Product features: 3 (n_ratings, mean_rating, std_rating)\")\n",
    "print(f\"   - Temporal features: 5 (year, month, weekday, days_since, recency_weight)\")\n",
    "print(f\"   - Interaction features: 4 (user/product/global deviations, z-score)\")\n",
    "print(f\"   - Normalized features: 8\")\n",
    "print(f\"   - Total: 23 features\")\n",
    "\n",
    "print(f\"\\n6. TEMPORAL RANGE:\")\n",
    "min_date = datetime.fromtimestamp(np.min(timestamps)).strftime('%Y-%m-%d')\n",
    "max_date = datetime.fromtimestamp(np.max(timestamps)).strftime('%Y-%m-%d')\n",
    "print(f\"   - From: {min_date}\")\n",
    "print(f\"   - To: {max_date}\")\n",
    "print(f\"   - Duration: {(np.max(timestamps) - np.min(timestamps)) / (365.25*24*3600):.1f} years\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[BƯỚC 8] Lưu dữ liệu đã xử lý...\")\n",
    "\n",
    "# Create output directory\n",
    "output_dir = '../data/processed/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save main data (vectorized I/O)\n",
    "np.savez_compressed(\n",
    "    os.path.join(output_dir, 'preprocessed_data.npz'),\n",
    "    user_indices=user_indices,\n",
    "    product_indices=product_indices,\n",
    "    ratings=ratings,\n",
    "    timestamps=timestamps,\n",
    "    # Original IDs\n",
    "    user_ids=user_ids,\n",
    "    product_ids=product_ids,\n",
    "    # User features\n",
    "    user_n_ratings=user_n_ratings,\n",
    "    user_mean_rating=user_mean_rating,\n",
    "    user_std_rating=user_std_rating,\n",
    "    # Product features\n",
    "    product_n_ratings=product_n_ratings,\n",
    "    product_mean_rating=product_mean_rating,\n",
    "    product_std_rating=product_std_rating,\n",
    "    # Temporal features\n",
    "    years=years,\n",
    "    months=months,\n",
    "    weekdays=weekdays,\n",
    "    days_since=days_since,\n",
    "    recency_weight=recency_weight,\n",
    "    # Interaction features\n",
    "    user_rating_deviation=user_rating_deviation,\n",
    "    product_rating_deviation=product_rating_deviation,\n",
    "    global_rating_deviation=global_rating_deviation,\n",
    "    user_rating_zscore=user_rating_zscore,\n",
    "    # Normalized features\n",
    "    user_n_ratings_norm=user_n_ratings_norm,\n",
    "    product_n_ratings_norm=product_n_ratings_norm,\n",
    "    days_since_norm=days_since_norm,\n",
    "    ratings_std=ratings_std,\n",
    "    user_mean_rating_std=user_mean_rating_std,\n",
    "    product_mean_rating_std=product_mean_rating_std,\n",
    "    user_n_ratings_robust=user_n_ratings_robust,\n",
    "    product_n_ratings_robust=product_n_ratings_robust\n",
    ")\n",
    "\n",
    "# Save mappings\n",
    "np.savez_compressed(\n",
    "    os.path.join(output_dir, 'id_mappings.npz'),\n",
    "    unique_users=unique_users_final,\n",
    "    unique_products=unique_products_final\n",
    ")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'n_records': len(ratings),\n",
    "    'n_users': n_users_final,\n",
    "    'n_products': n_products_final,\n",
    "    'sparsity': float(sparsity),\n",
    "    'global_mean_rating': float(global_mean_rating),\n",
    "    'min_user_ratings': MIN_USER_RATINGS,\n",
    "    'min_product_ratings': MIN_PRODUCT_RATINGS\n",
    "}\n",
    "\n",
    "np.save(os.path.join(output_dir, 'metadata.npy'), metadata)\n",
    "\n",
    "print(f\"✓ Đã lưu dữ liệu vào: {output_dir}\")\n",
    "print(f\"  Files:\")\n",
    "print(f\"    - preprocessed_data.npz (compressed, {os.path.getsize(os.path.join(output_dir, 'preprocessed_data.npz')) / 1024**2:.2f} MB)\")\n",
    "print(f\"    - id_mappings.npz\")\n",
    "print(f\"    - metadata.npy\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPROCESSING HOÀN TẤT!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nCác bước tiếp theo:\")\n",
    "print(\"  1. Exploratory Data Analysis (EDA) với features mới\")\n",
    "print(\"  2. Build Popularity-based Recommender\")\n",
    "print(\"  3. Build Collaborative Filtering (Item-based, User-based)\")\n",
    "print(\"  4. Build Matrix Factorization (SVD)\")\n",
    "print(\"  5. Evaluation và Comparison\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
