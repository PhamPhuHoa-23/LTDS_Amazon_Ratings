=== PRACTICAL IMPLEMENTATION TIPS FROM KAGGLE ===

## 1. DATA SAMPLING FOR DEVELOPMENT
- Full dataset: 2M+ rows (slow to iterate)
- Strategy: Use .head(10000) or .sample(100000) for development
- Develop pipeline on sample, then scale to full dataset
- Helps with faster debugging and experimentation

## 2. HANDLING SPARSITY
- Problem: 1.2M users × 249K products = 99.9% sparse
- Solutions:
  a) Filter users/products with min rating threshold (e.g., ≥5 ratings)
  b) Use smaller subset for utility matrix
  c) Sample 10K-15K ratings for collaborative filtering demos
  d) Focus on dense submatrix (active users × popular products)

## 3. UTILITY MATRIX CONSTRUCTION
- Format: rows=users, columns=products, values=ratings
- Fill strategy: fill_value=0 (most common)
- Alternative: fillna(0) after pivot
- Always TRANSPOSE before SVD: X = matrix.T
- Why? Want to find product-product correlations

## 4. SVD PARAMETERS
- n_components=10 is common starting point
- Trade-off: More components = more detail but slower
- Typical range: 10-50 components
- After SVD: Use np.corrcoef(decomposed_matrix)

## 5. ID MAPPING STRATEGY
- Original IDs: Strings (e.g., "A39HTATAQ9V7YF", "B001MA0QY2")
- Problem: Can't use as array indices
- Solution: Create mapping dictionaries
  ```
  user_id_map = {old_id: new_id for new_id, old_id in enumerate(user_ids)}
  product_id_map = {old_id: new_id for new_id, old_id in enumerate(product_ids)}
  ```
- Keep inverse mapping for displaying results
- Use LabelEncoder pattern (fit_transform / inverse_transform)

## 6. VISUALIZATION BEST PRACTICES
- Rating distribution: sns.countplot(x='Rating', data=data)
- Top products: .plot(kind='bar', figsize=(20,10))
- Remove x-tick labels for dense plots: ax.set_xticklabels([])
- Time series: plt.plot(values) with proper labels
- Heatmap: For small correlation matrices only (too large = unreadable)

## 7. TIMESTAMP HANDLING
- Format: Unix timestamp (seconds since 1970)
- Convert: pd.to_datetime(timestamps, unit='s')
- Extract: year, month, day, weekday for analysis
- For NumPy: Use datetime64 or manual calculation

## 8. PERFORMANCE OPTIMIZATIONS
- dropna() early to reduce data size
- Use vectorized operations (groupby, value_counts)
- Avoid iterating over rows (very slow)
- For large matrices: Consider chunked processing

## 9. RECOMMENDATION DISPLAY
- Show top 10 recommendations (standard)
- Include: Product ID + Rating Count + Avg Rating
- Present as DataFrame/table for readability
- Remove already-purchased items from recommendations

## 10. EVALUATION STRATEGY
- train_test_split: 80-20 or 90-10
- Metrics to track:
  * RMSE for rating prediction
  * Precision@K, Recall@K for ranking
  * Coverage: % of products recommended
  * Diversity: Variety in recommendations
- Compare multiple approaches (baseline vs advanced)

## 11. COMMON PITFALLS TO AVOID
- Don't forget to remove purchased item from recommendations
- Check for products with ratings in ALL intervals (fairness)
- Normalize features before weighted averaging
- Handle edge cases: Users with 1 rating, products with 1 rating
- Memory: Don't create full user-item matrix for 2M rows

## 12. NUMPY-SPECIFIC TIPS
- Use np.unique() for unique counts
- Use np.isin() for filtering
- Broadcasting for element-wise operations
- np.where() for conditional operations
- fancy indexing: array[array > threshold]
- Set operations: np.intersect1d, np.setdiff1d

## 13. PROGRESSIVE DEVELOPMENT
- Start simple: Popularity-based (baseline)
- Add complexity: Item-based CF
- Advanced: User-based CF
- Most advanced: Matrix factorization
- Each step should improve metrics

## 14. BUSINESS CONTEXT
- Part I: New users → Popularity-based
- Part II: Existing users → Collaborative filtering
- Part III: New products → Content-based (if metadata available)
- Hybrid: Combine multiple approaches

## 15. REPRODUCIBILITY
- Set random seeds: np.random.seed(42)
- Document data splits
- Save intermediate results
- Version control preprocessing steps
- Track hyperparameters


